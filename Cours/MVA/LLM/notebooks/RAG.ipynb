{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "CPGVVOEHZwdb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/nathanael/.local/lib/python3.10/site-packages (4.48.1)\n",
      "Requirement already satisfied: wikipedia in /home/nathanael/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nathanael/.local/lib/python3.10/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nathanael/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nathanael/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nathanael/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nathanael/.local/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nathanael/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4->wikipedia) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wikipedia\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(page_titles):\n",
    "    \"\"\"\n",
    "    Extracts Wikipedia pages and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_titles: A list of Wikipedia page titles to extract.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the text of each Wikipedia page.\n",
    "    \"\"\"\n",
    "\n",
    "    page_data = {}\n",
    "    for title in page_titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            content = page.content.strip()\n",
    "            content = content.replace(\"\\n\", \"\")\n",
    "            page_data[page.title] = content\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"Page '{title}' not found.\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"Disambiguation error for '{title}': {e.options}\")\n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = [\n",
    "               \"Roger Ap√©ry\",\n",
    "               \"Owen Willans Richardson\",\n",
    "               \"Otto Sackur\",\n",
    "               \"Ludvig Lorenz\",\n",
    "               \"Klaus von Klitzing\",\n",
    "               \"Henri Victor Regnault\",\n",
    "               \"Erwin Madelung\",\n",
    "              ]\n",
    "\n",
    "# Uncomment the next line to scroll through Wikipedia\n",
    "# wikipedia_data = extract_wikipedia_pages(page_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary using `json.dump()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wikipedia_data.json', 'w') as f:\n",
    "#     json.dump(wikipedia_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary using `json.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia_data.json', 'r') as f:\n",
    "    wikipedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107\n",
      "3455\n",
      "1683\n",
      "1873\n",
      "1762\n",
      "3431\n",
      "1487\n"
     ]
    }
   ],
   "source": [
    "for doc in wikipedia_data:\n",
    "    print(len(wikipedia_data[doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load just the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd9cdae6a504235aaa559506873470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32800a06224c0c98e16f2fbc0e5c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7f2ef21057466ea270a606fa36f1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3081cb91b864c97a301c918ef70f8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_max_length = tokenizer.model_max_length\n",
    "model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello [SEP] how are you? [SEP]'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode([\"hello\", \"how are you?\"])\n",
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01234',\n",
       " '34567',\n",
       " '67891',\n",
       " '91011',\n",
       " '11121',\n",
       " '21314',\n",
       " '14151',\n",
       " '51617',\n",
       " '17181',\n",
       " '819']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_splitting(text, chunk_length = 300, chunk_overlap = 100):\n",
    "    out = []\n",
    "    for i in range(0,len(text), chunk_length - chunk_overlap):\n",
    "        out.append(text[i: i + chunk_length])\n",
    "    return out\n",
    "\n",
    "text_splitting(\"\".join([str(x) for x in range(20)]), 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitting_paragraph(text, chunk_length = 300):\n",
    "    out = []\n",
    "    paragraph_list = text.split(\".\")\n",
    "    current_text = \"\"\n",
    "    length = 0\n",
    "    for par in paragraph_list:\n",
    "        if length != 0 and length + len(par) < chunk_length:\n",
    "            current_text += \". \" + par\n",
    "            length += len(par)\n",
    "        else:\n",
    "            if len(current_text) != 0:\n",
    "                out.append(current_text + \".\")\n",
    "            current_text = par\n",
    "            length = len(par)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Roger Ap√©ry (French: [ape Åi]; 14 November 1916, Rouen ‚Äì 18 December 1994, Caen) was a French mathematician most remembered for Ap√©ry's theorem, which states that Œ∂(3) is an irrational number.  Here, Œ∂(s) denotes the Riemann zeta function.\",\n",
       " '== Biography ==Ap√©ry was born in Rouen in 1916 to a French mother and Greek father.  His childhood was spent in Lille until 1926, when the family moved to Paris, where he studied at the Lyc√©e Ledru-Rollin and the Lyc√©e Louis-le-Grand.   He was admitted  at the √âcole normale sup√©rieure in 1935.',\n",
       " '  His studies were interrupted at the start of World War II; he was mobilized in September 1939, taken prisoner of war in June 1940, repatriated with pleurisy in June 1941, and hospitalized until August 1941.']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_data_splits = {}\n",
    "\n",
    "for doc in wikipedia_data.keys():\n",
    "    wikipedia_data_splits[doc] = text_splitting_paragraph(wikipedia_data[doc])\n",
    "\n",
    "first_key = page_titles[0]\n",
    "wikipedia_data_splits[first_key][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 14, 9.571428571428571)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_doc = min(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "max_doc = max(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "av_doc = sum(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits) / len(wikipedia_data_splits)\n",
    "\n",
    "min_doc,max_doc,av_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the embedder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e09cb88aee41f0835c7d648e1a9f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e70eae71934c3494d5d2536a773f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "output_dim = outputs.last_hidden_state.size(2)\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedder needs to know whether the document is a document or a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunk_list, doc_type=\"document\"):\n",
    "    encoded_docs = tokenizer([\"search_{}: {}\".format(doc_type, chunk) for chunk in chunk_list],\n",
    "                                 padding = True,\n",
    "                                 return_tensors=\"pt\")\n",
    "    output = model(**encoded_docs) # (batch, input_length, output_dim)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    output_embeddings = torch.sum(token_embeddings, 1)\n",
    "    output_embeddings = F.normalize(output_embeddings, p=2, dim=1)\n",
    "    return output_embeddings # (batch, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed([\"hello\", \"another document\", \"and another one\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger Ap√©ry 12\n",
      "Owen Richardson 14\n",
      "Otto Sackur 6\n",
      "Ludvig Lorenz 8\n",
      "Klaus von Klitzing 7\n",
      "Henri Victor Regnault 14\n",
      "Erwin Madelung 6\n"
     ]
    }
   ],
   "source": [
    "def populate_database(dic_splits, batch_size = 1):\n",
    "    n_chunks = sum([len(dic_splits[doc]) for doc in dic_splits])\n",
    "    vectorial_database = torch.zeros([n_chunks, output_dim], requires_grad = False).to(device)\n",
    "    chunk_list = []\n",
    "    n = 0\n",
    "    for doc in dic_splits.keys():\n",
    "        chunk_list_doc = dic_splits[doc]\n",
    "        print(doc, len(chunk_list_doc))\n",
    "        for i in range(0, len(chunk_list_doc), batch_size):\n",
    "            batch = chunk_list_doc[i:i+batch_size]\n",
    "            chunk_list += batch\n",
    "            embeddings = embed(batch, doc_type = \"document\")\n",
    "            vectorial_database[n:n+len(batch)] = embeddings\n",
    "            n += len(batch)\n",
    "    return chunk_list, vectorial_database\n",
    "\n",
    "# Uncomment this to populate the database\n",
    "chunk_list, vectorial_database = populate_database(wikipedia_data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectorial database using `torch.save()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vectorial_database, 'vectorial_database.pth')\n",
    "\n",
    "with open('chunk_list.json', 'w') as f:\n",
    "    json.dump(chunk_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database using `torch.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorial_database = torch.load('vectorial_database.pth')\n",
    "vectorial_database.to(device)\n",
    "vectorial_database.requires_grad_(False)\n",
    "\n",
    "with open('chunk_list.json', 'r') as f:\n",
    "    chunk_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, torch.Size([67, 384]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_list), vectorial_database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0557,  0.0384, -0.0307, -0.0143, -0.0188]) Roger Ap√©ry (French: [ape Åi]; 14 November 1916, Ro\n",
      "tensor([ 0.0187,  0.0690, -0.0273, -0.0288,  0.0369]) == Biography ==Ap√©ry was born in Rouen in 1916 to \n",
      "tensor([ 0.0271,  0.0228, -0.0974,  0.0471, -0.0042])   His studies were interrupted at the start of Wor\n",
      "tensor([-0.0875,  0.0111, -0.0129, -0.0130, -0.1274])  He wrote his doctoral thesis in algebraic geometr\n",
      "tensor([-0.0636,  0.0460, -0.0357,  0.1095, -0.0656])  In 1949 he was appointed Professor at the Univers\n",
      "tensor([-0.1005,  0.0247, -0.0105,  0.1190,  0.0525])  An indication of the difficulty is that the corre\n",
      "tensor([-0.1744,  0.0473,  0.0322,  0.0616,  0.0066])  Nevertheless, many mathematicians have since work\n",
      "tensor([ 0.0378,  0.0296, -0.0356,  0.0142,  0.0272]) Ap√©ry was active in politics and for a few years i\n",
      "tensor([-0.0365,  0.0051,  0.0233,  0.0197, -0.0063]) == Personal life ==Ap√©ry married in 1947 and had t\n",
      "tensor([-0.0262,  0.0947, -0.0276, -0.0283, -0.0226])  He was buried next to his parents at the P√®re Lac\n",
      "tensor([-0.1003,  0.0270,  0.0461,  0.0998, -0.0332])                     1        +                    \n",
      "tensor([-0.0699,  0.0599, -0.0151,  0.0524,  0.0431])  \"Roger Ap√©ry, 1916-1994: A Radical Mathematician\"\n",
      "tensor([-0.1280,  0.0374, -0.0519,  0.0955,  0.0142]) Sir Owen Willans Richardson (26 April 1879 ‚Äì 15 Fe\n",
      "tensor([-0.0384, -0.0073, -0.0479,  0.0775, -0.0003]) == Biography ==Richardson was born in Dewsbury, Yo\n",
      "tensor([-0.0943,  0.0029, -0.0153,  0.0672,  0.0005])  He then got a DSc from University College London \n",
      "tensor([-0.1631,  0.0333, -0.0570,  0.1261, -0.0278])  In 1901, he demonstrated that the current from a \n",
      "tensor([-0.0614,  0.0749, -0.0084,  0.0876, -0.0698])  This became known as Richardson's law: \"If then t\n",
      "tensor([-0.0752, -0.0443, -0.0122,  0.0639, -0.0128]) \"Richardson was professor at Princeton University \n",
      "tensor([-0.0138,  0.0190, -0.0107,  0.0661,  0.0049])   In 1927, he was one of the participants of the f\n",
      "tensor([-0.0982,  0.0068, -0.0371,  0.0788,  0.0094]) He also researched the photoelectric effect, the g\n"
     ]
    }
   ],
   "source": [
    "for i, embedding_vector in enumerate(vectorial_database[:20]):\n",
    "    print(embedding_vector[:5], chunk_list[i][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(query_embeddings, doc_embeddings):\n",
    "    return query_embeddings @ doc_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6542],\n",
      "        [0.4483]])\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = embed([\n",
    "    \"What is TSNE?\",\n",
    "    \"Who is Laurens van der Maaten?\",\n",
    "], \"query\")\n",
    "\n",
    "doc_embeddings = embed([\n",
    "    \"TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n",
    "], \"document\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(similarity(query_embeddings, doc_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5,\n",
    "             verbose = False):\n",
    "    query_embedding = embed([query], \"query\")\n",
    "    query_embedding.to(device)\n",
    "    similarity_scores = similarity(query_embedding, vectorial_database)\n",
    "    topk = torch.topk(similarity_scores, k = topk)\n",
    "\n",
    "    if verbose:\n",
    "        for score, idx in zip(topk.values[0], topk.indices[0]):\n",
    "            print(f\"Score: {score:.4f}\\nText:\\n\", chunk_list[idx], \"\\n\")\n",
    "    return \"\\n\".join([chunk_list[i] for i in topk.indices[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.57 ms ¬± 359 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "retrieve(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative retrieval: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "def retrieve_SVM(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5):\n",
    "    query_embedding = embed([query], \"query\")\n",
    "    x = np.concatenate([query_embedding.detach().numpy(), vectorial_database.detach().numpy()])\n",
    "    y = np.zeros(vectorial_database.size(0) + 1)\n",
    "    y[0] = 1 # we have a single positive example\n",
    "\n",
    "    clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1, dual=\"auto\")\n",
    "    clf.fit(x, y)\n",
    "    similarities = clf.decision_function(x)\n",
    "    sorted_ix = np.argsort(-similarities)\n",
    "    for k in sorted_ix[1:topk+1]:\n",
    "        print(f\"Score: {similarities[k]:.4f}\\nText:\\n\", chunk_list[k-1], \"\\n\")\n",
    "    return \"\\n\".join([chunk_list[k-1] for k in sorted_ix[1:topk+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.2089\n",
      "Text:\n",
      " Erwin Madelung (18 May 1881 ‚Äì 1 August 1972) was a German physicist. He was born in 1881 in Bonn.  His father was the surgeon Otto Wilhelm Madelung.  He earned a doctorate in 1905 from the University of G√∂ttingen, specializing in crystal structure, and eventually became a professor. \n",
      "\n",
      "Score: -0.3036\n",
      "Text:\n",
      "                     1        +                              1            8                          +                              1            27                          +                              1            64                          +        ‚ãØ        ‚â†                              p            q                                {\\displaystyle 1+{\\frac {1}{8}}+{\\frac {1}{27}}+{\\frac {1}{64}}+\\cdots \\neq {\\frac {p}{q}}}  == See also ==Ap√©ry's constantBasel problem== External links ==Ap√©ry, Fran√ßois (1996). \n",
      "\n",
      "Score: -0.3241\n",
      "Text:\n",
      "  He specialized in atomic physics and quantum mechanics, and it was during this time he developed the Madelung equations, an alternative form of the Schr√∂dinger equation. \n",
      "\n",
      "Score: -0.3370\n",
      "Text:\n",
      "  An indication of the difficulty is that the corresponding problem for other odd powers remains unsolved. \n",
      "\n",
      "Score: -0.3390\n",
      "Text:\n",
      " He is also known for the Madelung rule, which states that atomic orbitals are filled in order of increasing                     n        +        l              {\\displaystyle n+l}   quantum numbers. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Erwin Madelung (18 May 1881 ‚Äì 1 August 1972) was a German physicist. He was born in 1881 in Bonn.  His father was the surgeon Otto Wilhelm Madelung.  He earned a doctorate in 1905 from the University of G√∂ttingen, specializing in crystal structure, and eventually became a professor.\\n                    1        +                              1            8                          +                              1            27                          +                              1            64                          +        ‚ãØ        ‚â†                              p            q                                {\\\\displaystyle 1+{\\\\frac {1}{8}}+{\\\\frac {1}{27}}+{\\\\frac {1}{64}}+\\\\cdots \\\\neq {\\\\frac {p}{q}}}  == See also ==Ap√©ry's constantBasel problem== External links ==Ap√©ry, Fran√ßois (1996).\\n He specialized in atomic physics and quantum mechanics, and it was during this time he developed the Madelung equations, an alternative form of the Schr√∂dinger equation.\\n An indication of the difficulty is that the corresponding problem for other odd powers remains unsolved.\\nHe is also known for the Madelung rule, which states that atomic orbitals are filled in order of increasing                     n        +        l              {\\\\displaystyle n+l}   quantum numbers.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_SVM(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does **extractive** question answering, meaning it can only points to the answer in the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "QA = pipeline('question-answering', model=model_name, tokenizer=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt):\n",
    "    topk_chunks = retrieve(prompt)\n",
    "#     topk_chunks = retrieve_SVM(prompt)\n",
    "    return QA(question=prompt, context=topk_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6069597005844116,\n",
       " 'start': 303,\n",
       " 'end': 339,\n",
       " 'answer': 'atomic physics and quantum mechanics'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"What did Erwin Madelung study?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
