{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80517dbc",
   "metadata": {
    "id": "80517dbc"
   },
   "source": [
    "# GRPO Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DAEbh3jBadc7",
   "metadata": {
    "id": "DAEbh3jBadc7"
   },
   "source": [
    "In this notebook, you'll find:\n",
    "* A basic Transformer with basic tokenizer\n",
    "* A basic dataset for additions\n",
    "* A classical pre-trainer, minimizing cross entropy loss\n",
    "* A Vanilla GRPO\n",
    "* A PPO GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "id": "OzGh9ahKF17h"
   },
   "outputs": [],
   "source": [
    "num_digits = 3\n",
    "\n",
    "dataset_size = 64_000\n",
    "train_proportion = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd151a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fabd151a",
    "outputId": "e7b36965-8f38-4c49-cf03-0109ab723eca"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "id": "t6aC9uNeIR6C"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "\n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuCc6jF5F8hK",
    "outputId": "e42dddaa-dacd-471e-b7d0-48f17004dd87"
   },
   "outputs": [],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8FXW2K-1Jd-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FXW2K-1Jd-P",
    "outputId": "fff6c129-8820-4b3c-8caa-b43710370098"
   },
   "outputs": [],
   "source": [
    "prompt = \"12 + 42 =\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daa90f31",
    "outputId": "c23f0b74-6948-43a9-9e6f-5846f4b325b4"
   },
   "outputs": [],
   "source": [
    "def sample_datapoint(num_digits = 3):\n",
    "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    a_str = \"\".join([str(x) for x in a_list])\n",
    "    b_str = \"\".join([str(x) for x in b_list])\n",
    "    sum_int = a_int + b_int\n",
    "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6e861d2",
    "outputId": "7b639165-7809-4f2d-a067-f21547c293b1"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(num_digits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fee85050",
    "outputId": "69def39c-084c-4074-f296-908925038748"
   },
   "outputs": [],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91674239",
   "metadata": {
    "id": "91674239"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb278ab",
   "metadata": {
    "id": "4eb278ab"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp,\n",
    "                                               nhead=nhead,\n",
    "                                               dim_feedforward=nhid,\n",
    "                                               num_encoder_layers=nlayers)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output_enc = self.encoder(src, mask=self.src_mask)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return F.log_softmax(output_dec, dim=-1), output_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "7677a7ea-d06a-46a8-85f3-44274cc008a5"
   },
   "outputs": [],
   "source": [
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6PmJSo95N4C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6PmJSo95N4C",
    "outputId": "f0177342-43b6-46c6-b079-aab40db326d1"
   },
   "outputs": [],
   "source": [
    "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35d113",
   "metadata": {
    "id": "2e35d113"
   },
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f06e0",
   "metadata": {
    "id": "8f2f06e0"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
    "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device)\n",
    "    # (prompt_length, batch_size * num_samples)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
    "        logits = output[-1,:,:] # (batch_size * num_samples, ntokens)\n",
    "        if mode == \"greedy\":\n",
    "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
    "        else: # mode == \"sampling\"\n",
    "            logits /= temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
    "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d76d1b19",
    "outputId": "7c084e50-8269-4f24-859a-4f95f8bd143d"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00954ddc",
   "metadata": {
    "id": "00954ddc"
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c84beab",
    "outputId": "3e53a680-31e1-4cd6-8ce0-d96009def972"
   },
   "outputs": [],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f9227",
   "metadata": {
    "id": "264f9227"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i, batch_size):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "\n",
    "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
    "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
    "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
    "\n",
    "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
    "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
    "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
    "\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, prompt_length, answers_length, prompts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91e281ad",
    "outputId": "2a97779d-08e9-485a-9588-07e699dc6b77"
   },
   "outputs": [],
   "source": [
    "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
    "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KfmcSdPwp3K6",
   "metadata": {
    "id": "KfmcSdPwp3K6"
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcd10a",
   "metadata": {
    "id": "1cfcd10a"
   },
   "outputs": [],
   "source": [
    "def evaluate(batch_size = batch_size):\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
    "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size)\n",
    "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac335b05",
    "outputId": "1355d497-45f0-440a-90b8-30a7f2818091"
   },
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 5: Train the model, classical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827e567",
   "metadata": {
    "id": "b827e567"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b140ba3",
   "metadata": {
    "id": "5b140ba3"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 8e-4\n",
    "\n",
    "reporting_per_epoch = 5\n",
    "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
    "assert(log_interval % batch_size == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638a75d",
   "metadata": {
    "id": "3638a75d"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size)\n",
    "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size)\n",
    "            model.zero_grad()\n",
    "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens)\n",
    "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens)\n",
    "            target_answers = target_answers.view(-1)\n",
    "            loss = F.cross_entropy(output_answers, target_answers)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "        test_accuracy = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a8490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e2a8490",
    "outputId": "492562cc-d243-4314-ab56-d17d41c070ad"
   },
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d9d440",
    "outputId": "b876390d-4396-4a95-e600-e2230317dbec"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4c591",
   "metadata": {
    "id": "cfa4c591"
   },
   "source": [
    "## Step 6: Vanilla GRPO training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff83f72",
   "metadata": {
    "id": "aff83f72"
   },
   "source": [
    "### Custom reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548bf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c548bf7",
    "outputId": "b5d45345-22eb-445a-b888-6ad22e96d7b7"
   },
   "outputs": [],
   "source": [
    "def accuracy_reward(output, answer):\n",
    "    pattern = r\"\\[EOS\\]\"\n",
    "    output = re.sub(pattern, \"\", output)\n",
    "    pattern = r\"(\\[PAD\\])*$\"\n",
    "    output = re.sub(pattern, \"\", output)\n",
    "    return 1. if output == answer else 0.\n",
    "\n",
    "accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), accuracy_reward(\"123\", \"124\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f02762",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1f02762",
    "outputId": "7271bdf4-4ecf-44ee-ec86-8c815ffaa9a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distance_accuracy_reward(output, answer):\n",
    "    pattern = r\"\\[EOS\\]\"\n",
    "    output = re.sub(pattern, \"\", output)\n",
    "    pattern = r\"(\\[PAD\\])*$\"\n",
    "    output = re.sub(pattern, \"\", output)\n",
    "    int_output = int(output)\n",
    "    int_answer = int(answer)\n",
    "    return abs(int_output - int_answer) / max(int_output, int_answer)\n",
    "\n",
    "distance_accuracy_reward(\"123[EOS]\", \"123\"), distance_accuracy_reward(\"123[PAD]\", \"124\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a0d70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b42a0d70",
    "outputId": "a79ca9d6-a79b-4640-c172-4db78e433256"
   },
   "outputs": [],
   "source": [
    "def digit_accuracy_reward(output, answer):\n",
    "    pattern = r\"\\[EOS\\]\"\n",
    "    output = re.sub(pattern, \"\", output)\n",
    "    pattern = r\"(\\[PAD\\])*$\"\n",
    "    output = re.sub(pattern, \"\", output)\n",
    "    return sum(c1 == c2 for (c1,c2) in zip(output, answer)) / max(len(output), len(answer))\n",
    "\n",
    "digit_accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), digit_accuracy_reward(\"123[EOS]\", \"123\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41603b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a41603b2",
    "outputId": "33dbf431-4177-4ae3-d36c-08ff8a2261fc"
   },
   "outputs": [],
   "source": [
    "def reward_format(output):\n",
    "    pattern = r\"\\d+\\[EOS\\](\\[PAD\\])*$\"\n",
    "    return 1. if bool(re.match(pattern, output)) else 0.\n",
    "\n",
    "reward_format(\"123[EOS][PAD][PAD]\"), reward_format(\"123[EOS]\"), reward_format(\"123\"),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482e411",
   "metadata": {
    "id": "4482e411"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf764cb0",
   "metadata": {
    "id": "cf764cb0"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "num_samples = 16\n",
    "temperature = .8\n",
    "\n",
    "reporting_per_epoch = 5\n",
    "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
    "assert(log_interval % batch_size == 0)\n",
    "\n",
    "reward_fun = digit_accuracy_reward\n",
    "reward_format = reward_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cdced",
   "metadata": {
    "id": "f15cdced"
   },
   "outputs": [],
   "source": [
    "def compute_rewards(text_outputs, answers):\n",
    "    repeated_answers = [answer for answer in answers for _ in range(num_samples)]\n",
    "    rewards = torch.tensor(\n",
    "        [0.2 * reward_format(output) + 0.8 * reward_fun(output, answer)\n",
    "         for output, answer in zip(text_outputs, repeated_answers)],\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22be0d4",
   "metadata": {
    "id": "e22be0d4"
   },
   "outputs": [],
   "source": [
    "def calculate_grpo_advantages(rewards):\n",
    "    # reshape rewards to group by prompt\n",
    "    # compute mean and standard deviation for each prompt group\n",
    "    mean_rewards = rewards.view(-1, num_samples).mean(dim=1)\n",
    "    std_rewards = rewards.view(-1, num_samples).std(dim=1)\n",
    "    # expand the means and stds to match the original flat rewards tensor shape\n",
    "    mean_rewards = mean_rewards.repeat_interleave(num_samples, dim=0)\n",
    "    std_rewards = std_rewards.repeat_interleave(num_samples, dim=0)\n",
    "    # normalize rewards to get advantages\n",
    "    advantages = (rewards - mean_rewards) / (std_rewards + 1e-5)\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac5c99",
   "metadata": {
    "id": "faac5c99"
   },
   "outputs": [],
   "source": [
    "def compute_log_probs(model, outputs, prompt_length):\n",
    "    logits, _ = model(outputs)\n",
    "    # logits.shape = (prompt_length + answers_length + 1, batch_size * num_samples, vocab_size)\n",
    "\n",
    "    # we only need the log probabilities for the new tokens\n",
    "    # this introduces a shift: the logits for a position are the predictions for the next token\n",
    "    logits = logits[prompt_length-1:-1, :, :]\n",
    "    # logits.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
    "\n",
    "    # convert raw logits into log probabilities along the vocabulary axis\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2612214",
   "metadata": {
    "id": "b2612214"
   },
   "outputs": [],
   "source": [
    "def compute_loss(advantages, log_probs, responses):\n",
    "    # reshape responses from (answers_length + 1, batch_size * num_samples)\n",
    "    # to (answers_length + 1, batch_size * num_samples, 1) for gathering\n",
    "    responses = responses.unsqueeze(-1)\n",
    "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
    "    # responses.shape = (answers_length + 1, batch_size * num_samples)\n",
    "    # gather the log probability for each token in responses\n",
    "    selected_log_probs = log_probs.gather(dim=-1, index=responses)\n",
    "    # remove the extra last dimension to get back to shape (answers_length + 1, batch_size * num_samples).\n",
    "    selected_log_probs = selected_log_probs.squeeze(-1)\n",
    "\n",
    "    # normalize\n",
    "    selected_log_probs = (selected_log_probs - selected_log_probs.mean(-1, keepdim=True)) / (selected_log_probs.std(-1, keepdim=True) + 1e-5)\n",
    "\n",
    "    # advantages.shape = (batch_size * num_samples)\n",
    "    # we use the same advantages for all tokens in the response\n",
    "    loss = -(advantages.unsqueeze(dim=0) * selected_log_probs).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ca075",
   "metadata": {
    "id": "824ca075"
   },
   "outputs": [],
   "source": [
    "def train_vanilla_GRPO(verbose = False):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # switch eval for train model (enables dropout)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "\n",
    "            # get a batch of prompts and answers\n",
    "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "\n",
    "            # generate samples for each prompt\n",
    "            outputs = generate(model,\n",
    "                               prompts,\n",
    "                               new_tokens = answers_length + 1,\n",
    "                               mode = \"sampling\",\n",
    "                               num_samples = num_samples,\n",
    "                               temperature = temperature)\n",
    "            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
    "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
    "                            for i in range(outputs.size(1))]\n",
    "\n",
    "            # compute rewards\n",
    "            rewards = compute_rewards(text_outputs, answers)\n",
    "\n",
    "            # compute advantages\n",
    "            advantages = calculate_grpo_advantages(rewards)\n",
    "\n",
    "            # compute log probabilities\n",
    "            log_probs = compute_log_probs(model, outputs, prompt_length)\n",
    "\n",
    "            # compute loss\n",
    "            responses = outputs[prompt_length:, :]\n",
    "            loss = compute_loss(advantages, log_probs, responses)\n",
    "\n",
    "            # optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if i % log_interval == 0 and batch > 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
    "                if verbose:\n",
    "                    print(\"\\nquestion:\", questions[0],\n",
    "                      \"\\nanswer\", answers[0],\n",
    "                      \"\\noutput:\", text_outputs[:num_samples],\n",
    "                      \"\\nreward:\", rewards[:num_samples],\n",
    "                      \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
    "\n",
    "                start_time = time.time()\n",
    "        test_accuracy = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic_vanilla_GRPO.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02716ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "b02716ff",
    "outputId": "d3ae59c3-1d5f-4e40-e428-097a04b4edb1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_vanilla_GRPO(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeVn935w5BSp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeVn935w5BSp",
    "outputId": "2014648b-68e6-4af6-ce9c-b28a3d580c7e"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148574b",
   "metadata": {},
   "source": [
    "## Step 7: PPO GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5840d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_PPO(advantages, log_probs_current, log_probs_old, log_probs_ref, responses):\n",
    "    # reshape responses from (answers_length + 1, batch_size * num_samples)\n",
    "    # to (answers_length + 1, batch_size * num_samples, 1) for gathering\n",
    "    responses = responses.unsqueeze(-1)\n",
    "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
    "    # responses.shape = (answers_length + 1, batch_size * num_samples)\n",
    "    # gather the log probability for each token in responses\n",
    "    selected_log_probs_current = log_probs_current.gather(dim=-1, index=responses)\n",
    "    # remove the extra last dimension to get back to shape (answers_length + 1, batch_size * num_samples).\n",
    "    selected_log_probs_current = selected_log_probs_current.squeeze(-1)\n",
    "    # normalize\n",
    "    selected_log_probs_current = (selected_log_probs_current - selected_log_probs_current.mean(-1, keepdim=True)) / (selected_log_probs_current.std(-1, keepdim=True) + 1e-5)\n",
    "\n",
    "    # define selected_log_probs_old\n",
    "    selected_log_probs_old = log_probs_old.gather(dim=-1, index=responses)\n",
    "    selected_log_probs_old = selected_log_probs_old.squeeze(-1)\n",
    "    selected_log_probs_old = (selected_log_probs_old - selected_log_probs_old.mean(-1, keepdim=True)) / (selected_log_probs_old.std(-1, keepdim=True) + 1e-5)\n",
    "\n",
    "    # define selected_log_probs_ref\n",
    "    selected_log_probs_ref = log_probs_ref.gather(dim=-1, index=responses)\n",
    "    selected_log_probs_ref = selected_log_probs_ref.squeeze(-1)\n",
    "    selected_log_probs_ref = (selected_log_probs_ref - selected_log_probs_ref.mean(-1, keepdim=True)) / (selected_log_probs_ref.std(-1, keepdim=True) + 1e-5)\n",
    "\n",
    "    # compute KL divergence  with the same estimator (always positive as should be the KLD) as the deepseek paper\n",
    "    ratio_kl = selected_log_probs_current - selected_log_probs_ref\n",
    "    kl_per_token = ratio_kl.exp() - selected_log_probs_current + selected_log_probs_ref - 1.0\n",
    "    kl_div = kl_per_token.mean()\n",
    "    beta = 0.04\n",
    "\n",
    "    # advantages.shape = (batch_size * num_samples)\n",
    "    # we use the same advantages for all tokens in the response\n",
    "\n",
    "    # defining the PPO loss\n",
    "    eps = 0.2\n",
    "    ratio = (selected_log_probs_current - selected_log_probs_old).exp()\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - eps, 1 + eps)\n",
    "    res = torch.min( ratio*advantages.unsqueeze(dim=0), clipped_ratio*advantages.unsqueeze(dim=0) )\n",
    "    ppo_loss = -res.mean()\n",
    "    total_loss = ppo_loss + beta * kl_div\n",
    "    return total_loss, ppo_loss, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO_GRPO(verbose=False):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # switch eval for train model (enables dropout)\n",
    "    model.train()\n",
    "\n",
    "    # defining pi_ref\n",
    "    prompts, _, prompt_length, answers_length, _, _ = get_batch(\"train\", 0, batch_size)\n",
    "    prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "    outputs = generate(model,\n",
    "                       prompts,\n",
    "                       new_tokens = answers_length + 1,\n",
    "                       mode = \"sampling\",\n",
    "                       num_samples = num_samples,\n",
    "                       temperature = temperature)\n",
    "    log_probs_ref = compute_log_probs(model, outputs, prompt_length).detach()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        start_time = time.time()\n",
    "        # add tqdm later\n",
    "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "\n",
    "            # get a batch of prompts and answers\n",
    "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
    "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
    "\n",
    "            # generate samples for each prompt\n",
    "            outputs = generate(model,\n",
    "                               prompts,\n",
    "                               new_tokens = answers_length + 1,\n",
    "                               mode = \"sampling\",\n",
    "                               num_samples = num_samples,\n",
    "                               temperature = temperature)\n",
    "\n",
    "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
    "                            for i in range(outputs.size(1))]\n",
    "\n",
    "            # compute rewards\n",
    "            rewards = compute_rewards(text_outputs, answers)\n",
    "\n",
    "            # compute advantages\n",
    "            advantages = calculate_grpo_advantages(rewards)\n",
    "\n",
    "            # define pi_old\n",
    "            log_probs_old = compute_log_probs(model, outputs, prompt_length).detach()\n",
    "\n",
    "            # processing the same datapoint multiple times\n",
    "            for iteration in range(3):\n",
    "                # updating pi_theta\n",
    "                log_probs_current = compute_log_probs(model, outputs, prompt_length)\n",
    "                responses = outputs[prompt_length:, :]\n",
    "                loss, ppo_loss, kl = compute_loss_PPO(advantages, log_probs_current, log_probs_old, log_probs_ref, responses)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if i % log_interval == 0 and batch > 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
    "                print('accuracy', evaluate())\n",
    "                print('total loss', loss.item())\n",
    "                print('ppo loss', ppo_loss.item())\n",
    "                print('kl', kl.item())\n",
    "                if verbose:\n",
    "                    print(\"\\nquestion:\", questions[0],\n",
    "                      \"\\nanswer\", answers[0],\n",
    "                      \"\\noutput:\", text_outputs[:num_samples],\n",
    "                      \"\\nreward:\", rewards[:num_samples],\n",
    "                      \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "        # updating old log probs (pi_ref) at each epoch\n",
    "        log_probs_ref = log_probs_current.detach()\n",
    "        \n",
    "        test_accuracy = evaluate()\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic_PPO_GRPO.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PPO_GRPO(verbose=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
